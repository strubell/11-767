{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5301047d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "PRETRAINED_MODEL=\"prajjwal1/bert-small\"\n",
    "TASK=\"cola\"\n",
    "FINETUNED_PATH=\"bert-small-cola\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda7c325",
   "metadata": {},
   "source": [
    "##  Load Pretrained Model \n",
    "\n",
    "Code for loading a pretrained BERT checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68d6eea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at prajjwal1/bert-small were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-small and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizerFast\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(PRETRAINED_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b2267f",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Loads the dataset for GLUE task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b16dc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset glue (/home/patrick/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a76a4592c06b44eaa4dcd3d8ea5e28b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "\n",
    "dataset = load_dataset('glue', TASK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea0a6ce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ff987bf3fc4e7faf202ddc3e6801c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be5bd3351f304e1f899d272aeedddc83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ad2707eacc340bba5b6f60bfe0270c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "def tokenize_function(example):\n",
    "    # Tokenize the texts\n",
    "    args = (\n",
    "        (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
    "    )\n",
    "    result = tokenizer(*args, padding=padding, max_length=max_seq_length, truncation=True)\n",
    "\n",
    "    # Map labels to IDs (not necessary for GLUE tasks)\n",
    "    if label_to_id is not None and \"label\" in examples:\n",
    "        result[\"label\"] = [(label_to_id[l] if l != -1 else -1) for l in examples[\"label\"]]\n",
    "    return result\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8ff3d6",
   "metadata": {},
   "source": [
    "## Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1030f19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    metric = load_metric(\"glue\", \"mrpc\")\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8fdd63ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx.\n",
      "***** Running training *****\n",
      "  Num examples = 3668\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1377\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='1377' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  12/1377 00:04 < 09:36, 2.37 it/s, Epoch 0.02/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15643/202480259.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFINETUNED_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/11-767/lib/python3.7/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1371\u001b[0m                         \u001b[0moptimizer_was_run\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale_before\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mscale_after\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1372\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1373\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0moptimizer_was_run\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/11-767/lib/python3.7/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/11-767/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/11-767/lib/python3.7/site-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    347\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m                 \u001b[0;31m# In-place operations to update the averages at the same time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = FINETUNED_PATH, \n",
    "    evaluation_strategy=\"epoch\"\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb139b7",
   "metadata": {},
   "source": [
    "## Evaluating on Squad\n",
    "\n",
    "Code for running evaluation on finetuned model on squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "52da13bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file bert-small-mrpc/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"prajjwal1/bert-small\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file bert-small-mrpc/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at bert-small-mrpc.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "loading file https://huggingface.co/prajjwal1/bert-small/resolve/main/vocab.txt from cache at /home/patrick/.cache/huggingface/transformers/68be80309844e53b628e9d479926a991d0adf337752bb941f0188887240313b8.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/prajjwal1/bert-small/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/prajjwal1/bert-small/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/prajjwal1/bert-small/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/prajjwal1/bert-small/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/prajjwal1/bert-small/resolve/main/config.json from cache at /home/patrick/.cache/huggingface/transformers/ac031779e2b4dd1d9da1e39c9d6a29fd45deea195eb3703a701d9c77f60abb4e.1257bb8f1f585038e86954d2560e36ca5c2dd98a8cde30fd22468940c911b672\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/prajjwal1/bert-small/resolve/main/config.json from cache at /home/patrick/.cache/huggingface/transformers/ac031779e2b4dd1d9da1e39c9d6a29fd45deea195eb3703a701d9c77f60abb4e.1257bb8f1f585038e86954d2560e36ca5c2dd98a8cde30fd22468940c911b672\n",
      "Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizerFast\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(FINETUNED_PATH)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(PRETRAINED_MODEL)\n",
    "\n",
    "eval_args = TrainingArguments(\n",
    "    output_dir = FINETUNED_PATH,\n",
    "    do_train = False,\n",
    "    do_predict = True,\n",
    "    dataloader_drop_last = False    \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    eval_args,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1e6e9248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 1725\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6950724637681159, 'f1': 0.8094202898550724}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "start = timer()\n",
    "outputs = trainer.predict(tokenized_datasets[\"test\"])\n",
    "end = timer()\n",
    "delta = end-start\n",
    "metric = load_metric(\"glue\", TASK)\n",
    "predictions = np.argmax(outputs.predictions, axis=-1)\n",
    "metric.compute(predictions=predictions, references=outputs.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1e25a689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time per sample = 0.012398742895655205\n"
     ]
    }
   ],
   "source": [
    "print(f\"time per sample = {delta/len(tokenized_datasets['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113ac1a4",
   "metadata": {},
   "source": [
    "## Divide Checkpoint\n",
    "\n",
    "Code for dividing a checkpoint by layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51be41ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"ft-bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09acde2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from checkpoint_divider import divide_checkpoint\n",
    "\n",
    "divide_checkpoint(\"ft-bert-base-uncased/pytorch_model.bin\", divided_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aff4f0d",
   "metadata": {},
   "source": [
    "## Load Divided Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e30097e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_memory():\n",
    "    import os, psutil\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a051afcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {0: ['bert.embeddings.LayerNorm.bias', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.position_ids', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.word_embeddings.weight'], 1: ['bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.dense.weight'], 2: ['bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.dense.weight'], 3: ['bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.dense.weight'], 4: ['bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.dense.weight'], 5: ['bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.dense.weight'], 6: ['bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.dense.weight'], 7: ['bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.dense.weight'], 8: ['bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.dense.weight'], 9: ['bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.dense.weight'], 10: ['bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.dense.weight'], 11: ['bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.dense.weight'], 12: ['bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.dense.weight'], 13: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']})\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.load(f\"{divided_checkpoint}/layer2keys.bin\"))\n",
    "layers=[]\n",
    "memory=[]\n",
    "for layer in range(14):\n",
    "    layers.append(torch.load(f\"{divided_checkpoint}/pytorch_model_{layer}.bin\"))\n",
    "    memory.append(get_current_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a52e3052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe8797ca810>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e34212c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:11-767] *",
   "language": "python",
   "name": "conda-env-11-767-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
