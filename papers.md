Tuesday, September 7
----
**Lecture:** Practical applications

**Readings:**
- [Natural Language Processing with Small Feed-Forward Networks](https://arxiv.org/abs/1708.00214)
- [Machine Learning at Facebook: Understanding Inference at the Edge](https://research.fb.com/wp-content/uploads/2018/12/Machine-Learning-at-Facebook-Understanding-Inference-at-the-Edge.pdf)
- [Recognizing People in Photos Through Private On-Device Machine Learning](https://machinelearning.apple.com/research/recognizing-people-photos)
- [Knowledge Transfer for Efficient On-device False Trigger Mitigation](https://arxiv.org/abs/2010.10591)

Tuesday, September 14
----
**Lecture:** Compression I (Distillation and pruning)

**Readings:**
- [Model Compression](https://www.cs.cornell.edu/~caruana/compression.kdd06.pdf)
- [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531)
- [DistilBERT](https://arxiv.org/abs/1910.01108)
- [TernaryBERT: Distillation-aware Ultra-low Bit BERT](https://www.aclweb.org/anthology/2020.emnlp-main.37/)
- [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://www.aclweb.org/anthology/2020.acl-main.195/)
- [Distilling Large Language Models into Tiny and Effective Students using pQRNN](https://arxiv.org/abs/2101.08890)
- [The curious case of developmental BERTology: On sparsity, transfer learning, generalization and the brain](https://arxiv.org/abs/2007.03774)
- [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/abs/1803.03635)
- [Sequence-Level Knowledge Distillation](https://arxiv.org/abs/1606.07947)


Thursday, September 21
----
**Lecture:** 
Neural architecture search

**Readings:**
- [SpArSe: Sparse Architecture Search for CNNs on Resource-Constrained Microcontrollers](https://arxiv.org/abs/1905.12107)
- [FBNet: Hardware-Aware Efficient ConvNet Design via Differentiable NAS](https://arxiv.org/abs/1812.03443)
- [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946)
- [High-Performance Large-Scale Image Recognition Without Normalization](https://arxiv.org/abs/2102.06171)
- [HAT: Hardware-Aware Transformers for Efficient Natural Language Processing](https://arxiv.org/abs/2005.14187)


Tuesday, September 28
----
**Lecture:** Benchmarking. 

**Readings:**
- [Show Your Work: Improved Reporting of Experimental Results](https://aclanthology.org/D19-1224/)
- [Showing Your Work Doesn’t Always Work](https://aclanthology.org/2020.acl-main.246/)
- [The Hardware Lottery](https://arxiv.org/abs/2009.06489)
- [HULK: An Energy Efficiency Benchmark Platform for Responsible Natural Language Processing](https://arxiv.org/abs/2002.05829)
- [An Analysis of Deep Neural Network Models for Practical Applications](https://arxiv.org/abs/1605.07678)
- [MLPerf Inference Benchmark](https://arxiv.org/abs/1911.02549)
- [MLPerf Training Benchmark](https://arxiv.org/abs/1910.01500)
- [Roofline: an insightful visual performance model for multicore architectures](https://people.eecs.berkeley.edu/~kubitron/cs252/handouts/papers/RooflineVyNoYellow.pdf)
- [Evaluating the Energy Efficiency of Deep Convolutional Neural Networks on CPUs and GPUs](https://ieeexplore.ieee.org/document/7723730)
- [Deep Learning Language Modeling Workloads: Where Time Goes on Graphics Processors](https://ieeexplore.ieee.org/document/9041972)

Tuesday, October 5
----
**Lecture:** Compression II (Pruning and quantization)

**Readings:**
- [Binarized Neural Networks](https://arxiv.org/abs/1602.02830)
- [Training Deep Neural Networks with 8-bit Floating Point Numbers](https://arxiv.org/abs/1812.08011)
- [HAWQ: Hessian AWare Quantization of Neural Networks with Mixed-Precision](https://arxiv.org/abs/1905.03696)
- [Bayesian Bits: Unifying Quantization and Pruning](https://arxiv.org/abs/2005.07093)
- [Scalable Methods for 8-bit Training of Neural Networks](https://arxiv.org/abs/1805.11046)
- [Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers](https://arxiv.org/abs/2002.11794)
- [Once-for-All: Train One Network and Specialize it for Efficient Deployment](https://arxiv.org/abs/1908.09791)
- [Q-BERT: Hessian Based Ultra Low Precision Quantization of BERT](https://arxiv.org/abs/1909.05840)
- [I-BERT: Integer-only BERT Quantization](https://arxiv.org/abs/2101.01321)
- Also, see pruning readings listed under September 14.


Tuesday, October 12
----
**Lecture:** Multimodal fusion.

**Readings:**
- [VisualBERT: A Simple and Performant Baseline for Vision and Language](https://arxiv.org/abs/1908.03557)
- [Mapping Navigation Instructions to Cont. Control Actions with Position-Visitation Pred.](https://arxiv.org/abs/1811.04179)
- [Early Fusion for Goal Directed Robotic Vision](https://arxiv.org/abs/1811.08824)


Tuesday, October 19
----
**Lecture:** Architecture-specific tricks I: CNNs

**Readings:**
- [XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks](https://arxiv.org/abs/1603.05279)
- [XOR-Net: An Efficient Computation Pipeline for Binary Neural Network Inference on Edge Devices](https://ieeexplore.ieee.org/document/9359148)
- [MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications](https://arxiv.org/abs/1704.04861)
- [Fast Convolutional Nets With fbfft: A GPU Performance Evaluation](https://arxiv.org/abs/1412.7580)
- [FFT Convolutions are Faster than Winograd on Modern CPUs, Here’s Why](https://arxiv.org/abs/1809.07851)
- [Fast Algorithms for Convolutional Neural Networks](https://arxiv.org/abs/1509.09308)
- [Efficient softmax approximation for GPUs](https://arxiv.org/abs/1609.04309)
- [Adaptive Input Representations for Neural Language Modeling](https://arxiv.org/abs/1809.1085)
- [A Study of Non-autoregressive Model for Sequence Generation](https://arxiv.org/abs/2004.10454)
- [Mask-Predict: Parallel Decoding of Conditional Masked Language Models](https://arxiv.org/abs/1904.09324)
- [Non-autoregressive neural machine translation](https://arxiv.org/abs/1711.02281)



Tuesday, October 26
----
**Lecture:** Architecture-specific tricks II: Transformers

**Readings:**
- [Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing](https://arxiv.org/abs/2006.03236)
- [Do Transformer Modifications Transfer Across Implementations and Applications?](https://arxiv.org/abs/2102.11972)
- [A Primer in BERTology: What we know about how BERT works](https://arxiv.org/abs/2002.12327)
- [Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732)
- [Improving Low Compute Language Modeling with In-Domain Embedding Initialisation](https://arxiv.org/abs/2009.14109)
- [Consistent Accelerated Inference via Confident Adaptive Transformers](https://arxiv.org/abs/2104.08803)
- [PoWER-BERT: Accelerating BERT Inference via Progressive Word-vector Elimination](https://arxiv.org/abs/2001.08950)
- [Are Larger Pretrained Language Models Uniformly Better? Comparing Performance at the Instance Level](https://arxiv.org/abs/2105.06020)
- [Are Sixteen Heads Really Better Than One?](http://papers.nips.cc/paper/9551-are-sixteen-heads-really-better-than-one)


Tuesday, November 2
----
**Lecture:** Guest lecture on efficient speech processing by [Shinji Watanabe](https://sites.google.com/view/shinjiwatanabe).

**Readings:**
- [Speech recognition with deep recurrent neural networks](https://ieeexplore.ieee.org/abstract/document/6638947)
- [Streaming End-to-end Speech Recognition For Mobile Devices](https://arxiv.org/abs/1811.06621)


Tuesday, November 9
----
**Lecture:** Accelerating training.

**Readings:**
- [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/abs/2101.00190)
- [Pre-Training Transformers as Energy-Based Cloze Models](https://arxiv.org/abs/2012.08561)
- [Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/abs/1902.00751)
- [Accelerating Deep Learning by Focusing on the Biggest Losers](https://arxiv.org/abs/1910.00762)
- [Dataset Distillation](https://arxiv.org/abs/1811.10959)
- [Competence-based curriculum learning for neural machine translation](https://arxiv.org/abs/1903.09848)
- [Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks](https://arxiv.org/abs/1703.03400)

Tuesday, November 16
----
**Lecture:** Paper presentations. 

**Readings:**
Your choice, drawn from this list and/or related to your project. Must be proposed by October 12.

Tuesday, November 23
----
**Lecture:** Paper presentations. 

**Readings:**
Your choice, drawn from this list and/or related to your project. Must be proposed by October 12.

Tuesday, November 30
----
**Lecture:** Carbon footprint and alternative power sources.

**Readings:**
- [Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning
](https://jmlr.org/papers/v21/20-312.html)
- [Tackling Climate Change with Machine Learning](https://arxiv.org/abs/1906.05433)
- [IrEne: Interpretable Energy Prediction for Transformers](https://arxiv.org/abs/2106.01199)
- [On the opportunities and risks of foundation models (Section 5.3)](https://arxiv.org/abs/2108.07258)
- [Carbontracker: Tracking and Predicting the Carbon Footprint of Training Deep Learning Models](https://arxiv.org/abs/2007.03051)
- [Quantifying the Carbon Emissions of Machine Learning](https://arxiv.org/abs/1910.09700)


